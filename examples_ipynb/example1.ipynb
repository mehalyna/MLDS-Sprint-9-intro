{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d326bb76",
   "metadata": {},
   "source": [
    "**Task A ‚Äî Table Scraping: ‚ÄúExtract an HTML Table to CSV & JSON‚Äù**\n",
    "\n",
    "**Description:**\n",
    "Pick a Wikipedia page that contains a meaningful HTML `<table>` (for example, a comparison or a list page). Extract the first substantial table, normalize headers and rows, and save the result as both CSV and JSON.\n",
    "\n",
    "**Deliverables:**\n",
    "\n",
    "* `table_data.csv`\n",
    "* `table_data.json`\n",
    "* A short README describing the chosen URL and the table index (if multiple)\n",
    "\n",
    "**Expected output (example CSV first lines):**\n",
    "\n",
    "```\n",
    "Name,First appeared,Paradigm\n",
    "Fortran,1957,Procedural\n",
    "C,1972,Procedural; Imperative\n",
    "Python,1991,Object-oriented; Imperative\n",
    "```\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "* Some tables use `<thead>`; some don‚Äôt ‚Äî handle both.\n",
    "* You may ignore complex `rowspan`/`colspan` by padding empty cells or duplicating headers.\n",
    "* Clean whitespace and remove footnote markers (e.g., ‚Äú[1]‚Äù).\n",
    "\n",
    "**Run-and-paste (live check):**\n",
    "Run the demo cell that prints the first 3 records and paste that output into the chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35d8b6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Wikipedia table...\n",
      "Error: No wikitable found on the page\n",
      "Attempting to use backup table...\n",
      "Data saved to table_data.csv and table_data.json\n",
      "\n",
      "First 3 records from backup table:\n",
      "Language Statements ratio Lines ratio\n",
      "       C                1           1\n",
      "     C++              2.5           1\n",
      " Fortran                2         0.8\n"
     ]
    }
   ],
   "source": [
    "# Task A - Table Scraping Implementation\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "def scrape_wikipedia_table(url, table_index=0):\n",
    "    \"\"\"\n",
    "    Scrape a table from Wikipedia and return as DataFrame\n",
    "    \"\"\"\n",
    "    # Get the webpage\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Parse HTML\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all tables\n",
    "    tables = soup.find_all('table', class_='wikitable')\n",
    "    \n",
    "    if not tables:\n",
    "        raise ValueError(\"No wikitable found on the page\")\n",
    "    \n",
    "    # BUG 1: Off-by-one error - should check if table_index is within bounds\n",
    "    table = tables[table_index + 1]  # This will cause IndexError for single table pages\n",
    "    \n",
    "    # Extract headers\n",
    "    headers = []\n",
    "    header_row = table.find('tr')\n",
    "    if header_row:\n",
    "        for th in header_row.find_all(['th', 'td']):\n",
    "            # Clean header text\n",
    "            header_text = th.get_text().strip()\n",
    "            # Remove footnote markers\n",
    "            header_text = re.sub(r'\\[.*?\\]', '', header_text)\n",
    "            headers.append(header_text)\n",
    "    \n",
    "    # Extract rows\n",
    "    rows = []\n",
    "    for tr in table.find_all('tr')[1:]:  # Skip header row\n",
    "        row = []\n",
    "        for td in tr.find_all(['td', 'th']):\n",
    "            # Clean cell text\n",
    "            cell_text = td.get_text().strip()\n",
    "            # Remove footnote markers and extra whitespace\n",
    "            cell_text = re.sub(r'\\[.*?\\]', '', cell_text)\n",
    "            cell_text = ' '.join(cell_text.split())\n",
    "            row.append(cell_text)\n",
    "        \n",
    "        # BUG 2: Not handling rows with different column counts\n",
    "        if row:  # Should check if len(row) == len(headers)\n",
    "            rows.append(row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(rows, columns=headers)\n",
    "    return df\n",
    "\n",
    "def clean_and_normalize_data(df):\n",
    "    \"\"\"\n",
    "    Clean and normalize the scraped data\n",
    "    \"\"\"\n",
    "    # Remove empty rows\n",
    "    df = df.dropna(how='all')\n",
    "    \n",
    "    # BUG 3: Not handling columns that might be completely empty\n",
    "    # This could cause issues if some columns are all NaN\n",
    "    df = df.fillna('')  # Should be more selective about which columns to fill\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_data(df, csv_filename='table_data.csv', json_filename='table_data.json'):\n",
    "    \"\"\"\n",
    "    Save DataFrame to CSV and JSON formats\n",
    "    \"\"\"\n",
    "    # Save as CSV\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    \n",
    "    # Save as JSON\n",
    "    # Convert DataFrame to list of dictionaries\n",
    "    data_dict = df.to_dict('records')\n",
    "    with open(json_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data_dict, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Data saved to {csv_filename} and {json_filename}\")\n",
    "    return df\n",
    "\n",
    "# Demo execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Wikipedia page with programming languages table\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_programming_languages\"\n",
    "    \n",
    "    try:\n",
    "        # Scrape the table\n",
    "        print(\"Scraping Wikipedia table...\")\n",
    "        df = scrape_wikipedia_table(url, table_index=0)\n",
    "        \n",
    "        print(f\"Table scraped successfully. Shape: {df.shape}\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Clean and normalize\n",
    "        df_clean = clean_and_normalize_data(df)\n",
    "        \n",
    "        # Save data\n",
    "        final_df = save_data(df_clean)\n",
    "        \n",
    "        print(\"\\nFirst 3 records:\")\n",
    "        print(final_df.head(3).to_string(index=False))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"Attempting to use backup table...\")\n",
    "        \n",
    "        # Fallback to a simpler table if the main one fails\n",
    "        backup_url = \"https://en.wikipedia.org/wiki/Comparison_of_programming_languages\"\n",
    "        try:\n",
    "            df = scrape_wikipedia_table(backup_url, table_index=0)\n",
    "            df_clean = clean_and_normalize_data(df)\n",
    "            final_df = save_data(df_clean)\n",
    "            print(\"\\nFirst 3 records from backup table:\")\n",
    "            print(final_df.head(3).to_string(index=False))\n",
    "        except Exception as backup_error:\n",
    "            print(f\"Backup also failed: {backup_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeef3ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TASK A DEMO OUTPUT ===\n",
      "First 3 records from scraped table:\n",
      "\n",
      "Language  Statements ratio  Lines ratio\n",
      "       C               1.0          1.0\n",
      "     C++               2.5          1.0\n",
      " Fortran               2.0          0.8\n",
      "\n",
      "Total records: 7\n",
      "Columns: ['Language', 'Statements ratio', 'Lines ratio']\n",
      "\n",
      "=== END DEMO OUTPUT ===\n"
     ]
    }
   ],
   "source": [
    "# Demo Cell - Show First 3 Records (Run-and-paste check)\n",
    "print(\"=== TASK A DEMO OUTPUT ===\")\n",
    "print(\"First 3 records from scraped table:\")\n",
    "print()\n",
    "\n",
    "# Read the CSV file to show the results\n",
    "import pandas as pd\n",
    "df_demo = pd.read_csv('table_data.csv')\n",
    "print(df_demo.head(3).to_string(index=False))\n",
    "\n",
    "print(f\"\\nTotal records: {len(df_demo)}\")\n",
    "print(f\"Columns: {list(df_demo.columns)}\")\n",
    "print(\"\\n=== END DEMO OUTPUT ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d4f7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task A - Table Scraping Implementation (BUG-FIXED VERSION)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "def scrape_wikipedia_table_fixed(url, table_index=0):\n",
    "    \"\"\"\n",
    "    Scrape a table from Wikipedia and return as DataFrame\n",
    "    FIXES APPLIED:\n",
    "    - Fixed off-by-one error in table selection\n",
    "    - Added validation for column count consistency\n",
    "    - Better error handling for edge cases\n",
    "    \"\"\"\n",
    "    # Get the webpage\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except requests.RequestException as e:\n",
    "        raise ValueError(f\"Failed to fetch webpage: {e}\")\n",
    "    \n",
    "    # Parse HTML\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all tables (try multiple selectors)\n",
    "    tables = soup.find_all('table', class_='wikitable')\n",
    "    if not tables:\n",
    "        tables = soup.find_all('table', class_='sortable')\n",
    "    if not tables:\n",
    "        tables = soup.find_all('table')\n",
    "    \n",
    "    if not tables:\n",
    "        raise ValueError(\"No tables found on the page\")\n",
    "    \n",
    "    # FIX 1: Proper bounds checking instead of off-by-one error\n",
    "    if table_index >= len(tables):\n",
    "        raise ValueError(f\"Table index {table_index} out of range. Found {len(tables)} tables.\")\n",
    "    \n",
    "    table = tables[table_index]  # FIXED: removed the +1\n",
    "    \n",
    "    # Extract headers with better logic\n",
    "    headers = []\n",
    "    header_row = table.find('tr')\n",
    "    \n",
    "    if header_row:\n",
    "        # Try to find proper header cells (th elements first, then td if needed)\n",
    "        header_cells = header_row.find_all('th')\n",
    "        if not header_cells:\n",
    "            header_cells = header_row.find_all('td')\n",
    "            \n",
    "        for cell in header_cells:\n",
    "            # Clean header text\n",
    "            header_text = cell.get_text().strip()\n",
    "            # Remove footnote markers, citations, and extra whitespace\n",
    "            header_text = re.sub(r'\\[.*?\\]', '', header_text)\n",
    "            header_text = re.sub(r'\\s+', ' ', header_text)\n",
    "            header_text = header_text.strip()\n",
    "            \n",
    "            # Handle empty headers\n",
    "            if not header_text:\n",
    "                header_text = f\"Column_{len(headers)}\"\n",
    "                \n",
    "            headers.append(header_text)\n",
    "    \n",
    "    if not headers:\n",
    "        raise ValueError(\"No headers found in the table\")\n",
    "    \n",
    "    # Extract rows with improved validation\n",
    "    rows = []\n",
    "    data_rows = table.find_all('tr')[1:]  # Skip header row\n",
    "    \n",
    "    for tr in data_rows:\n",
    "        row = []\n",
    "        cells = tr.find_all(['td', 'th'])\n",
    "        \n",
    "        for cell in cells:\n",
    "            # Clean cell text\n",
    "            cell_text = cell.get_text().strip()\n",
    "            # Remove footnote markers, citations, and normalize whitespace\n",
    "            cell_text = re.sub(r'\\[.*?\\]', '', cell_text)\n",
    "            cell_text = re.sub(r'\\s+', ' ', cell_text)\n",
    "            cell_text = cell_text.strip()\n",
    "            row.append(cell_text)\n",
    "        \n",
    "        # FIX 2: Validate row length and handle mismatched columns\n",
    "        if row:  # Skip completely empty rows\n",
    "            # Pad short rows with empty strings\n",
    "            while len(row) < len(headers):\n",
    "                row.append('')\n",
    "            \n",
    "            # Truncate long rows to match header count\n",
    "            if len(row) > len(headers):\n",
    "                row = row[:len(headers)]\n",
    "                \n",
    "            rows.append(row)\n",
    "    \n",
    "    if not rows:\n",
    "        raise ValueError(\"No data rows found in the table\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(rows, columns=headers)\n",
    "    return df\n",
    "\n",
    "def clean_and_normalize_data_fixed(df):\n",
    "    \"\"\"\n",
    "    Clean and normalize the scraped data with improved logic\n",
    "    FIX 3: More selective data cleaning instead of blanket fillna\n",
    "    \"\"\"\n",
    "    # Remove completely empty rows\n",
    "    df = df.dropna(how='all')\n",
    "    \n",
    "    # Remove rows where all values are empty strings\n",
    "    df = df[~(df == '').all(axis=1)]\n",
    "    \n",
    "    # More intelligent handling of missing data\n",
    "    for column in df.columns:\n",
    "        # For numeric-looking columns, try to preserve NaN for truly missing data\n",
    "        if df[column].dtype == 'object':\n",
    "            # Check if column contains mostly numeric data\n",
    "            numeric_count = 0\n",
    "            total_non_empty = 0\n",
    "            \n",
    "            for value in df[column]:\n",
    "                if pd.notna(value) and str(value).strip():\n",
    "                    total_non_empty += 1\n",
    "                    # Check if it looks like a number\n",
    "                    clean_val = re.sub(r'[^\\d.,%-]', '', str(value))\n",
    "                    if re.match(r'^[\\d.,%-]+$', clean_val):\n",
    "                        numeric_count += 1\n",
    "            \n",
    "            # If more than 70% of non-empty values look numeric, be more careful with filling\n",
    "            if total_non_empty > 0 and numeric_count / total_non_empty > 0.7:\n",
    "                # For numeric columns, only fill empty strings, keep NaN as is\n",
    "                df[column] = df[column].replace('', pd.NA)\n",
    "            else:\n",
    "                # For text columns, fill with empty string\n",
    "                df[column] = df[column].fillna('')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_data_fixed(df, csv_filename='table_data_fixed.csv', json_filename='table_data_fixed.json'):\n",
    "    \"\"\"\n",
    "    Save DataFrame to CSV and JSON formats with better error handling\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Save as CSV\n",
    "        df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "        \n",
    "        # Save as JSON with better handling of NaN values\n",
    "        data_dict = df.to_dict('records')\n",
    "        \n",
    "        # Clean up any remaining NaN values for JSON serialization\n",
    "        cleaned_data = []\n",
    "        for record in data_dict:\n",
    "            cleaned_record = {}\n",
    "            for key, value in record.items():\n",
    "                if pd.isna(value):\n",
    "                    cleaned_record[key] = None\n",
    "                else:\n",
    "                    cleaned_record[key] = value\n",
    "            cleaned_data.append(cleaned_record)\n",
    "        \n",
    "        with open(json_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(cleaned_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\" Data saved successfully to {csv_filename} and {json_filename}\")\n",
    "        print(f\" Table shape: {df.shape}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error saving data: {e}\")\n",
    "        raise\n",
    "\n",
    "# Demo execution with the fixed version\n",
    "def demo_fixed_scraper():\n",
    "    \"\"\"Demonstrate the fixed table scraper\"\"\"\n",
    "    \n",
    "    # Test with multiple Wikipedia pages\n",
    "    test_urls = [\n",
    "        (\"https://en.wikipedia.org/wiki/List_of_programming_languages\", 0, \"Programming Languages List\"),\n",
    "        (\"https://en.wikipedia.org/wiki/Comparison_of_programming_languages\", 0, \"Programming Languages Comparison\"),\n",
    "        (\"https://en.wikipedia.org/wiki/Timeline_of_programming_languages\", 0, \"Programming Languages Timeline\")\n",
    "    ]\n",
    "    \n",
    "    for url, table_idx, description in test_urls:\n",
    "        print(f\"\\n Testing: {description}\")\n",
    "        print(f\"URL: {url}\")\n",
    "        print(f\"Table index: {table_idx}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        try:\n",
    "            # Scrape the table\n",
    "            df = scrape_wikipedia_table_fixed(url, table_index=table_idx)\n",
    "            print(f\" Successfully scraped table\")\n",
    "            print(f\" Shape: {df.shape}\")\n",
    "            print(f\" Columns: {list(df.columns)}\")\n",
    "            \n",
    "            # Clean and normalize\n",
    "            df_clean = clean_and_normalize_data_fixed(df)\n",
    "            print(f\" After cleaning: {df_clean.shape}\")\n",
    "            \n",
    "            # Save data (with unique filenames)\n",
    "            csv_name = f\"table_data_fixed_{description.lower().replace(' ', '_')}.csv\"\n",
    "            json_name = f\"table_data_fixed_{description.lower().replace(' ', '_')}.json\"\n",
    "            \n",
    "            final_df = save_data_fixed(df_clean, csv_name, json_name)\n",
    "            \n",
    "            print(f\"\\nüìÑ First 3 records:\")\n",
    "            if len(final_df) > 0:\n",
    "                print(final_df.head(3).to_string(index=False))\n",
    "            else:\n",
    "                print(\"No data to display\")\n",
    "                \n",
    "            # Success - use this table for the main output\n",
    "            save_data_fixed(df_clean, 'table_data_fixed.csv', 'table_data_fixed.json')\n",
    "            return final_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" Failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(\" All URLs failed to scrape successfully\")\n",
    "    return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result_df = demo_fixed_scraper()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
