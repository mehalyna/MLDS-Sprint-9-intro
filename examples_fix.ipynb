{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca16d4f5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Task A - Table Scraping Implementation\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "def scrape_wikipedia_table(url, table_index=0):\n",
    "    \"\"\"\n",
    "    Scrape a table from Wikipedia and return as DataFrame\n",
    "    \"\"\"\n",
    "    # Get the webpage\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Parse HTML\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all tables\n",
    "    tables = soup.find_all('table', class_='wikitable')\n",
    "    \n",
    "    if not tables:\n",
    "        raise ValueError(\"No wikitable found on the page\")\n",
    "    \n",
    "    # BUG 1: Off-by-one error - should check if table_index is within bounds\n",
    "    table = tables[table_index + 1]  # This will cause IndexError for single table pages\n",
    "    \n",
    "    # Extract headers\n",
    "    headers = []\n",
    "    header_row = table.find('tr')\n",
    "    if header_row:\n",
    "        for th in header_row.find_all(['th', 'td']):\n",
    "            # Clean header text\n",
    "            header_text = th.get_text().strip()\n",
    "            # Remove footnote markers\n",
    "            header_text = re.sub(r'\\[.*?\\]', '', header_text)\n",
    "            headers.append(header_text)\n",
    "    \n",
    "    # Extract rows\n",
    "    rows = []\n",
    "    for tr in table.find_all('tr')[1:]:  # Skip header row\n",
    "        row = []\n",
    "        for td in tr.find_all(['td', 'th']):\n",
    "            # Clean cell text\n",
    "            cell_text = td.get_text().strip()\n",
    "            # Remove footnote markers and extra whitespace\n",
    "            cell_text = re.sub(r'\\[.*?\\]', '', cell_text)\n",
    "            cell_text = ' '.join(cell_text.split())\n",
    "            row.append(cell_text)\n",
    "        \n",
    "        # BUG 2: Not handling rows with different column counts\n",
    "        if row:  # Should check if len(row) == len(headers)\n",
    "            rows.append(row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(rows, columns=headers)\n",
    "    return df\n",
    "\n",
    "def clean_and_normalize_data(df):\n",
    "    \"\"\"\n",
    "    Clean and normalize the scraped data\n",
    "    \"\"\"\n",
    "    # Remove empty rows\n",
    "    df = df.dropna(how='all')\n",
    "    \n",
    "    # BUG 3: Not handling columns that might be completely empty\n",
    "    # This could cause issues if some columns are all NaN\n",
    "    df = df.fillna('')  # Should be more selective about which columns to fill\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_data(df, csv_filename='table_data.csv', json_filename='table_data.json'):\n",
    "    \"\"\"\n",
    "    Save DataFrame to CSV and JSON formats\n",
    "    \"\"\"\n",
    "    # Save as CSV\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    \n",
    "    # Save as JSON\n",
    "    # Convert DataFrame to list of dictionaries\n",
    "    data_dict = df.to_dict('records')\n",
    "    with open(json_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data_dict, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Data saved to {csv_filename} and {json_filename}\")\n",
    "    return df\n",
    "\n",
    "# Demo execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Wikipedia page with programming languages table\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_programming_languages\"\n",
    "    \n",
    "    try:\n",
    "        # Scrape the table\n",
    "        print(\"Scraping Wikipedia table...\")\n",
    "        df = scrape_wikipedia_table(url, table_index=0)\n",
    "        \n",
    "        print(f\"Table scraped successfully. Shape: {df.shape}\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Clean and normalize\n",
    "        df_clean = clean_and_normalize_data(df)\n",
    "        \n",
    "        # Save data\n",
    "        final_df = save_data(df_clean)\n",
    "        \n",
    "        print(\"\\nFirst 3 records:\")\n",
    "        print(final_df.head(3).to_string(index=False))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"Attempting to use backup table...\")\n",
    "        \n",
    "        # Fallback to a simpler table if the main one fails\n",
    "        backup_url = \"https://en.wikipedia.org/wiki/Comparison_of_programming_languages\"\n",
    "        try:\n",
    "            df = scrape_wikipedia_table(backup_url, table_index=0)\n",
    "            df_clean = clean_and_normalize_data(df)\n",
    "            final_df = save_data(df_clean)\n",
    "            print(\"\\nFirst 3 records from backup table:\")\n",
    "            print(final_df.head(3).to_string(index=False))\n",
    "        except Exception as backup_error:\n",
    "            print(f\"Backup also failed: {backup_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324dde9d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
